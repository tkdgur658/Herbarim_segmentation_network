{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b271229-3629-424a-9e5c-9dcc531af0a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:11:49.822212Z",
     "iopub.status.busy": "2024-10-16T01:11:49.822140Z",
     "iopub.status.idle": "2024-10-16T01:11:49.860206Z",
     "shell.execute_reply": "2024-10-16T01:11:49.859960Z",
     "shell.execute_reply.started": "2024-10-16T01:11:49.822204Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "import natsort\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import random\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# import ipynbname\n",
    "FILENAME = os.getcwd()+'/'+str(__session__).split('/')[-1]\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import RandomResizedCrop\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from monai.losses import TverskyLoss as TverskyLoss\n",
    "from monai.transforms import Compose, ToTensor, RandFlip\n",
    "from monai.metrics import DiceMetric as Dice_Function\n",
    "from monai.metrics import compute_iou as IoU_Function\n",
    "from monai.metrics import ConfusionMatrixMetric\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "model_dir = 'models'\n",
    "module_names = ['PSA_UNet']\n",
    "Dataset_dir = '240925_Herbarium_Dataset' \n",
    "data_split_csv='Herbarium_dataset_split.csv'\n",
    "model_names = module_names\n",
    "\n",
    "for  module_name in module_names:\n",
    "    exec(f'from {model_dir}.{module_name} import *')\n",
    "\n",
    "iterations = [1, 10]\n",
    "# train_size=0.6\n",
    "\n",
    "in_channels = 3\n",
    "number_of_classes=1\n",
    "epochs = 50 # 125\n",
    "EARLY_STOP = 25  \n",
    "batch_size = 16\n",
    "\n",
    "devices = [0,1]\n",
    "\n",
    "optimizer = 'AdamW'\n",
    "lr = 1e-3\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "optim_args = {'optimizer': optimizer, 'lr': lr, 'momentum': momentum, 'weight_decay': weight_decay}\n",
    "\n",
    "lr_scheduler = 'CosineAnnealingLR'\n",
    "T_max = epochs\n",
    "T_0 = epochs\n",
    "eta_min = 1e-6\n",
    "lr_scheduler_args = {'lr_scheduler': lr_scheduler, 'T_max': T_max, 'T_0': T_0, 'eta_min': eta_min}\n",
    "\n",
    "loss_function = 'DiceBCELoss'\n",
    "# loss_function = 'Tversky Focal Loss'\n",
    "reduction = 'mean'\n",
    "gamma = 2.0\n",
    "weight = None\n",
    "loss_function_args = {'loss_function': loss_function, 'reduction': reduction, 'gamma': gamma, 'weight': weight}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ec659d-2ef6-4323-8dc2-7a0105a5732d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:11:49.860849Z",
     "iopub.status.busy": "2024-10-16T01:11:49.860618Z",
     "iopub.status.idle": "2024-10-16T01:11:49.878861Z",
     "shell.execute_reply": "2024-10-16T01:11:49.878652Z",
     "shell.execute_reply.started": "2024-10-16T01:11:49.860840Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def control_random_seed(seed, pytorch=True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available()==True:\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "    except:\n",
    "        pass\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "def imread_kor ( filePath, mode=cv2.IMREAD_UNCHANGED ) : \n",
    "    stream = open( filePath.encode(\"utf-8\") , \"rb\") \n",
    "    bytes = bytearray(stream.read()) \n",
    "    numpyArray = np.asarray(bytes, dtype=np.uint8)\n",
    "    return cv2.imdecode(numpyArray , mode)\n",
    "def imwrite_kor(filename, img, params=None): \n",
    "    try: \n",
    "        ext = os.path.splitext(filename)[1] \n",
    "        result, n = cv2.imencode(ext, img, params) \n",
    "        if result:\n",
    "            with open(filename, mode='w+b') as f: \n",
    "                n.tofile(f) \n",
    "                return True\n",
    "        else: \n",
    "            return False \n",
    "    except Exception as e: \n",
    "        print(e) \n",
    "        return False\n",
    "    \n",
    "def random_rotation(image, mask, angle_range=(-30, 30)):\n",
    "    angle = random.uniform(angle_range[0], angle_range[1])\n",
    "    image = TF.rotate(image, angle)\n",
    "    mask = TF.rotate(mask, angle)\n",
    "    return image, mask\n",
    "\n",
    "class ImagesDataset(Dataset):\n",
    "    def __init__(self, image_path_list, target_path_list, aug=False):\n",
    "        self.image_path_list = image_path_list\n",
    "        self.target_path_list = target_path_list\n",
    "        self.transform = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                ])\n",
    "        self.aug = aug\n",
    "    def __len__(self):\n",
    "        return len(self.image_path_list)\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_path_list[idx]\n",
    "        mask_path = self.target_path_list[idx]\n",
    "        image = imread_kor(image_path)\n",
    "        image = self.transform(image).float()\n",
    "        \n",
    "        mask = imread_kor(mask_path)\n",
    "        mask = np.where(mask >= 128, 1, 0)\n",
    "        mask = self.transform(mask).float()\n",
    "        if self.aug==True:\n",
    "            if random.random() < 0.5:\n",
    "                resize_transform = RandomResizedCrop(size=(384, 256))\n",
    "                i, j, h, w = resize_transform.get_params(image, scale=(0.7, 1.0), ratio=(1, 1))\n",
    "                image = TF.resized_crop(image, i, j, h, w, (384, 256))\n",
    "            if random.random() < 0.5:\n",
    "                image = RandFlip(1, 0)(image)\n",
    "                mask = RandFlip(1, 0)(mask)\n",
    "            if random.random() < 0.5:\n",
    "                image, mask = random_rotation(image, mask)\n",
    "        return image, mask, image_path\n",
    "\n",
    "def Pixel_Accuracy(yhat, ytrue, threshold=0.5):\n",
    "    yhat = yhat>threshold\n",
    "    correct = torch.sum(yhat == ytrue)\n",
    "    total = ytrue.numel()\n",
    "    accuracy = correct.float() / total\n",
    "    return accuracy.item()\n",
    "\n",
    "def Intersection_over_Union(yhat, ytrue, threshold=0.5):\n",
    "    yhat = yhat>threshold\n",
    "    return IoU_Function(yhat, ytrue).nanmean().item()\n",
    " \n",
    "def Dice_Coefficient(yhat, ytrue, threshold=0.5):\n",
    "    yhat = yhat>threshold\n",
    "    return Dice_Function()(yhat, ytrue).nanmean().item()\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        Dice_BCE = BCE + dice_loss\n",
    "        \n",
    "        return Dice_BCE\n",
    "    \n",
    "def Confusion_Matrix(yhat, ytrue, threshold=0.5):\n",
    "    yhat = yhat>threshold\n",
    "    confusion_matrix = ConfusionMatrixMetric(metric_name = [\"recall\", \"precision\"], reduction ='mean', compute_sample =True)\n",
    "    confusion_matrix(yhat, ytrue)\n",
    "    recall, precision = confusion_matrix.aggregate()\n",
    "    return recall, precision\n",
    "    \n",
    "def train(train_loader, epoch, \\\n",
    "          model, criterion, optimizer, device\n",
    "          ):\n",
    "    model.train()\n",
    "    train_losses=AverageMeter()\n",
    "    for i, (input, target, _) in enumerate(train_loader):\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "        output = nn.Sigmoid()(model(input))\n",
    "        loss = criterion(output,target).float()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.update(loss.detach().cpu().numpy(),input.shape[0])\n",
    "    Train_Loss=np.round(train_losses.avg,6)\n",
    "    return Train_Loss\n",
    "def validate(validation_loader, \n",
    "          model, criterion, device,\n",
    "        model_path=False,\n",
    "             return_image_paths=False,\n",
    "          ):\n",
    "    if model_path!=False:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    for i, (input, target, image_path) in enumerate(validation_loader):\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = nn.Sigmoid()(model(input))\n",
    "        if i==0:\n",
    "            targets=target\n",
    "            outputs=output\n",
    "            if return_image_paths==True:\n",
    "                image_paths = image_path\n",
    "        else:\n",
    "            targets=torch.cat((targets,target))\n",
    "            outputs=torch.cat((outputs,output),axis=0)\n",
    "            if return_image_paths==True:\n",
    "                image_paths += image_path\n",
    "    if return_image_paths==True:\n",
    "        return outputs, targets, image_paths\n",
    "    return outputs, targets\n",
    "\n",
    "def str_to_class(classname):\n",
    "    return getattr(sys.modules[__name__], classname)\n",
    "\n",
    "def copy_sourcefile(output_dir, src_dir = 'src' ):    \n",
    "    import os \n",
    "    import shutil\n",
    "    import glob \n",
    "    source_dir = os.path.join(output_dir, src_dir)\n",
    "\n",
    "    os.makedirs(source_dir, exist_ok=True)\n",
    "    org_files1 = os.path.join('./', '*.py' )\n",
    "    org_files2 = os.path.join('./', '*.sh' )\n",
    "    org_files3 = os.path.join('./', '*.ipynb' )\n",
    "    org_files4 = os.path.join('./', '*.txt' )\n",
    "    org_files5 = os.path.join('./', '*.json' )    \n",
    "    files =[]\n",
    "    files = glob.glob(org_files1 )\n",
    "    files += glob.glob(org_files2  )\n",
    "    files += glob.glob(org_files3  )\n",
    "    files += glob.glob(org_files4  ) \n",
    "    files += glob.glob(org_files5  )     \n",
    "\n",
    "    # print(\"COPY source to output/source dir \", files)\n",
    "    tgt_files = os.path.join( source_dir, '.' )\n",
    "    for i, file in enumerate(files):\n",
    "        shutil.copy(file, tgt_files)\n",
    "class LossSaver(object):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "    def reset(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "    def update(self, train_loss, val_loss):\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "    def return_list(self):\n",
    "        return self.train_losses, self.val_losses\n",
    "    def save_as_csv(self, csv_file):\n",
    "        df = pd.DataFrame({'Train Losses': self.train_losses, 'Validation Losses': self.val_losses})\n",
    "        df.index = [f\"{i+1} Epoch\" for i in df.index]\n",
    "        df.to_csv(csv_file, index=True)\n",
    "class AverageMeter (object):\n",
    "    def __init__(self):\n",
    "        self.reset ()\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count        \n",
    "def create_dataset_lists(Dataset_dir, iteration, data_split_csv):\n",
    "    df = pd.read_csv(data_split_csv)\n",
    "    \n",
    "    split_name = f'split{str(iteration).zfill(2)}'  \n",
    "    \n",
    "    train_data = df[(df['split'] == split_name) & (df['data_type'] == 'training')]\n",
    "    validation_data = df[(df['split'] == split_name) & (df['data_type'] == 'validation')]\n",
    "    test_data = df[(df['split'] == split_name) & (df['data_type'] == 'test')]\n",
    "    \n",
    "    train_image_path_list = [os.path.join(Dataset_dir, path) for path in train_data['image'].tolist()]\n",
    "    train_target_path_list = [os.path.join(Dataset_dir, path) for path in train_data['mask'].tolist()]\n",
    "    \n",
    "    validation_image_path_list = [os.path.join(Dataset_dir, path) for path in validation_data['image'].tolist()]\n",
    "    validation_target_path_list = [os.path.join(Dataset_dir, path) for path in validation_data['mask'].tolist()]\n",
    "    \n",
    "    test_image_path_list = [os.path.join(Dataset_dir, path) for path in test_data['image'].tolist()]\n",
    "    test_target_path_list = [os.path.join(Dataset_dir, path) for path in test_data['mask'].tolist()]\n",
    "    \n",
    "    \n",
    "    return (\n",
    "        train_image_path_list, train_target_path_list, \n",
    "        validation_image_path_list, validation_target_path_list, \n",
    "        test_image_path_list, test_target_path_list\n",
    "    )\n",
    "\n",
    "def Do_Experiment(iteration, model_name, model, train_loader, validation_loader, test_loader, Optimizer, lr,  number_of_classes, epochs, Metrics,df,device, transform):\n",
    "    start = timeit.default_timer()\n",
    "    train_bool=True\n",
    "    test_bool=True\n",
    "    if loss_function == 'Tversky Focal Loss':\n",
    "        criterion=TverskyLoss()\n",
    "    elif loss_function == 'DiceBCELoss':\n",
    "        criterion=DiceBCELoss()\n",
    "    if Optimizer=='Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif Optimizer == 'SGD':\n",
    "        momentum = 0.9\n",
    "        weight_decay = 1e-4\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum ,weight_decay=weight_decay)\n",
    "    elif Optimizer =='AdamW':\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    if lr_scheduler_args['lr_scheduler'] == 'CosineAnnealingLR':\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = lr_scheduler_args['T_max'], eta_min = lr_scheduler_args['eta_min'])\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "    control_random_seed(seed)\n",
    "    if train_bool:\n",
    "        now = datetime.now()\n",
    "        Train_date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "        print('Training Start Time:',Train_date)\n",
    "        best=9999\n",
    "        best_epoch=1\n",
    "        Early_Stop=0\n",
    "        loss_saver = LossSaver()\n",
    "        train_start_time = timeit.default_timer()\n",
    "        for epoch in range(1, epochs+1):\n",
    "            Train_Loss = train(train_loader, epoch, \n",
    "              model, criterion, optimizer, device\n",
    "              )\n",
    "            lr_scheduler.step()\n",
    "            outputs, targets  \\\n",
    "            = validate(validation_loader, \n",
    "              model, criterion, device\n",
    "              )\n",
    "            Val_Loss = np.round(criterion(outputs,targets).cpu().numpy(),6)            \n",
    "            iou = np.round(Intersection_over_Union(outputs, targets),3)\n",
    "            dice = np.round(Dice_Coefficient(outputs, targets),3)\n",
    "            now = datetime.now()\n",
    "            date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "            print(str(epoch)+'EP('+date+'):',end=' ')\n",
    "            print('T_Loss: ' + str(Train_Loss), end=' ')\n",
    "            print('V_Loss: ' + str(Val_Loss), end=' ')\n",
    "            print('IoU: ' + str(iou), end=' ')\n",
    "            print('Dice: ' + str(dice), end=' ')\n",
    "            \n",
    "            loss_saver.update(Train_Loss, Val_Loss)\n",
    "            loss_saver.save_as_csv(f'{output_dir}/Losses_{Experiments_Time}.csv')\n",
    "            if Val_Loss<best:\n",
    "                Early_Stop = 0\n",
    "                torch.save(model.state_dict(), f'{output_dir}/{Train_date}_{model_name}_Iter_{iteration}.pt')\n",
    "                best_epoch = epoch\n",
    "                best = Val_Loss\n",
    "                print('Best Epoch:',best_epoch,'Loss:',Val_Loss)\n",
    "            else:\n",
    "                print('')\n",
    "                Early_Stop+=1\n",
    "            if Early_Stop>=EARLY_STOP:\n",
    "                break\n",
    "        train_stop_time = timeit.default_timer()\n",
    "    if test_bool:\n",
    "        now = datetime.now()\n",
    "        date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "        print('Test Start Time:',date)\n",
    "        outputs, targets, image_paths \\\n",
    "            = validate(test_loader, \n",
    "              model, criterion, device,\n",
    "            model_path=f'{output_dir}/{Train_date}_{model_name}_Iter_{iteration}.pt',\n",
    "                       return_image_paths=True\n",
    "              )        \n",
    "        Loss = np.round(criterion(outputs,targets).cpu().numpy(),6)\n",
    "        pa = np.round(Pixel_Accuracy(outputs.cpu(), targets.cpu()),3)\n",
    "        iou = np.round(Intersection_over_Union(outputs, targets),3)\n",
    "        dice = np.round(Dice_Coefficient(outputs, targets),3)\n",
    "        recall, precision = Confusion_Matrix(outputs, targets) \n",
    "        recall = np.round(recall.cpu().numpy()[0],3); precision = np.round(precision.cpu().numpy()[0],3)\n",
    "                \n",
    "        now = datetime.now()\n",
    "        date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "        print('Best Epoch:',best_epoch)\n",
    "        print('Test('+date+'): '+'Loss: ' + str(Loss),end=' ')\n",
    "        print('PA: ' + str(pa), end=' ')\n",
    "        print('IoU: ' + str(iou), end=' ')\n",
    "        print('Dice: ' + str(dice), end=' ')\n",
    "        print('Recall: ' + str(recall), end=' ')\n",
    "        print('Precision: ' + str(precision), end=' ')\n",
    "                            \n",
    "        stop = timeit.default_timer();m, s = divmod((train_stop_time - train_start_time)/epoch, 60);h, m = divmod(m, 60);Time_per_Epoch = \"%02d:%02d:%02d\" % (h, m, s);\n",
    "        m, s = divmod(stop - start, 60);h, m = divmod(m, 60);Time = \"%02d:%02d:%02d\" % (h, m, s);\n",
    "        total_params = sum(p.numel() for p in model.parameters()); total_params = format(total_params , ',');\n",
    "        Performances = [Experiments_Time, Train_date, iteration, model_name, best, Loss, pa, iou, dice, recall, precision, total_params,Time, best_epoch, Time_per_Epoch, loss_function, lr, batch_size, epochs, FILENAME]\n",
    "        df = df.append(pd.Series(Performances, index=df.columns), ignore_index=True)\n",
    "        os.makedirs(f'{output_dir}/test_outputs', exist_ok = True)\n",
    "        outputs = outputs.cpu().numpy()\n",
    "        for output, image_path in zip(outputs, image_paths):\n",
    "            np.save(f'{output_dir}/test_outputs/{os.path.basename(image_path)}', output)\n",
    "    now = datetime.now()\n",
    "    date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "    print('End',date)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691908be-458c-416b-8edd-1e07c042414d",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-10-16T01:11:49.879293Z",
     "iopub.status.busy": "2024-10-16T01:11:49.879194Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Start Time: 241016_101149\n",
      "ObsNet_Remove_att (Iter 2)\n",
      "Training Start Time: 241016_101150\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "Experiments_Time=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "print('Experiment Start Time:',Experiments_Time)\n",
    "Metrics=['Experiment Time','Train Time', 'Iteration','Model Name', 'Val_Loss', 'Test_Loss','PA', 'IoU', 'Dice', 'Recall', 'Precision', 'Total Params','Train-Predction Time','Best Epoch','Time per Epoch', 'Loss Function', 'LR', 'Batch size', '#Epochs', 'DIR']\n",
    "df = pd.DataFrame(index=None, columns=Metrics)\n",
    "output_root = f'output/output_{Experiments_Time}'\n",
    "os.makedirs(output_root, exist_ok = True)\n",
    "    \n",
    "for iteration in range(iterations[0], iterations[1]+1):\n",
    "    seed = iteration\n",
    "    control_random_seed(seed)\n",
    "    (train_image_path_list, train_target_path_list,\n",
    "     validation_image_path_list, validation_target_path_list,\n",
    "     test_image_path_list, test_target_path_list) = create_dataset_lists(Dataset_dir, iteration, data_split_csv)\n",
    "    \n",
    "    # train_image_path_list = natsort.natsorted(train_image_path_list[:100])\n",
    "    # train_target_path_list = natsort.natsorted(train_target_path_list[:100])\n",
    "    # validation_image_path_list  = natsort.natsorted(validation_image_path_list[:100])\n",
    "    # validation_target_path_list = natsort.natsorted(validation_target_path_list[:100])\n",
    "    # test_image_path_list = natsort.natsorted(test_image_path_list[:100])\n",
    "    # test_target_path_list= natsort.natsorted(test_target_path_list[:100])\n",
    "    \n",
    "    train_dataset = ImagesDataset(train_image_path_list, train_target_path_list, aug=True)\n",
    "    validation_dataset = ImagesDataset(validation_image_path_list, validation_target_path_list, aug=False)\n",
    "    test_dataset = ImagesDataset(test_image_path_list, test_target_path_list, aug=False)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size,\n",
    "    num_workers=4, pin_memory=True, shuffle=True, drop_last=True,\n",
    "    )\n",
    "    validation_loader = torch.utils.data.DataLoader(\n",
    "        validation_dataset, batch_size=batch_size, \n",
    "        num_workers=4, pin_memory=True,\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=batch_size, \n",
    "        num_workers=4, pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        print(f'{model_name} (Iter {iteration})')\n",
    "        output_dir = output_root + f'/{model_name}_Iter_{iteration}'\n",
    "        copy_sourcefile(output_dir, src_dir='src')\n",
    "        control_random_seed(seed)\n",
    "        model=str_to_class(model_name)(in_channels, number_of_classes)\n",
    "        device = torch.device(\"cuda:\"+str(devices[0]))\n",
    "        if len(devices)>1:\n",
    "            model = torch.nn.DataParallel(model, device_ids = devices ).to(device)\n",
    "        else:\n",
    "            model = model.to(device)\n",
    "        df = Do_Experiment(seed, model_name, model, train_loader, validation_loader, test_loader,  optimizer, lr,  number_of_classes, epochs, Metrics, df, device,None)\n",
    "        try:\n",
    "            df.to_csv(output_root+'/'+'Plant_Seg_'+Experiments_Time+'.csv', index=False, header=True, encoding=\"cp949\")\n",
    "        except:\n",
    "            now = datetime.now()\n",
    "            tmp_date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "            df.to_csv(output_root+'/'+'Plant_Seg_'+Experiments_Time+'_'+tmp_date+'_tmp'+'.csv', index=False, header=True, encoding=\"cp949\")\n",
    "import os\n",
    "print('End')\n",
    "os._exit(00) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44627737-1da2-473c-90f7-9ded7c16c400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_2.0",
   "language": "python",
   "name": "torch_2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
